---
title: 'Biostatistics Task 2'
author: 'Danyu Zhang & Daniel Alonso'
date: 'May 28th, 2021'
output: 'pdf_document'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
collapse = TRUE,
comment = '#>',
fig.path = './figures/'
)
knitr::knit_engines$set(julia = JuliaCall::eng_juliacall)
options(JULIA_HOME = '/home/dreth/julia/bin')
```

```{r, echo=TRUE, warning=FALSE, message=FALSE}
library(ggplot2)
library(survival)
library(ggfortify)
library(coin)
library(MASS)
library(survminer)
```

## Exercise 1

```{r, echo=FALSE, warning=FALSE, message=FALSE}
range <- seq(1,10, length.out=1000)
hazard <- sapply(range, function(t) {ifelse(t > 5, 0.4, 0.07)})
theta <- sapply(hazard, function(h) {1/h})
cumulative_hazard <- range/theta
survival <- (1/theta)*exp(-1*cumulative_hazard)
df <- data.frame(time=range, hazard=hazard, cumulative_hazard=cumulative_hazard, survival=survival)
```

### Hazard function plot

We have a piecewise hazard function as follows:

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=5}
ggplot(data=df, aes(x=time, y=hazard)) + geom_point() + ggtitle('Hazard function')
```

### Survival plot

Given that the survival function must be a smooth function, we obtain a survival function 

We obtain a piecewise survival function whose first chunk corresponds 

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=5}
ggplot(data=df, aes(x=time, y=survival)) + geom_point() + ggtitle('Survival function')
```

### Survival times simulation

\small

```{r, echo=TRUE, warning=FALSE, message=FALSE}
# number of trials
survival_times <- sapply(theta, function(lambda) {rexp(1,rate=1/lambda)})
```

\normalsize

### Histogram for survival times

We plot a histogram for the survival times

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=5}
hist(survival_times)
```

### Median survival time

As we can see, sampling from the distribution results in the following median survival time:

```{r, echo=FALSE, warning=FALSE, message=FALSE}
median(survival_times)
```

## Exercise 2

Given the following density function:

$$f(y) = (\lambda_0 + \lambda_1 y) e^{-\lambda_0 y - \frac{1}{2} \lambda_1 y^2}$$

We obtain the survival function as follows:

$$\begin{split} S(t) = P(T > t) & = \int_{t}^{\infty} (\lambda_0 + \lambda_1 y) e^{-\lambda_0 y - \frac{1}{2} \lambda_1 y^2} dy \\
& = \lim_{b\to\infty} [- e^{\frac{\lambda_1 b^2}{2} - \lambda_0 b}] + e^{\frac{-\lambda_1 t^2}{2}- \lambda_0 t} \\
& = 0 + e^{\frac{-\lambda_1 t^2}{2}- \lambda_0 t} \\
S(t) & = e^{\frac{-\lambda_1 t^2}{2}- \lambda_0 t}, \text{ } \lambda_1 \in \mathbb{R}, \lambda_0 > 0 \end{split}$$

We obtain the hazard function as follows:

$$h(t) = \frac{f(t)}{S(t)} = \frac{(\lambda_0 + \lambda_1 t) e^{-\lambda_0 t - \frac{1}{2} \lambda_1 t^2}}{e^{-\lambda_0 t - \frac{1}{2} \lambda_1 t^2}} = \lambda_0 + \lambda_1 t$$

$$h(t) = \lambda_0 + \lambda_1 t$$

And the cumulative hazard function:

$$H(t) = -log(S(t)) = \frac{\lambda_1 t^2}{2} + \lambda_0 t$$

## Exercise 3

### KM estimator implementation of the survival function

Our implementation is as follows:

#### Parameters:

- **dataset**: Dataset to obtain the KM estimation from

- **events**: specific column of the dataset corresponding to the events (deemed *status* for the *aml* dataset)

#### Algorithm:

- Obtain length of the dataset by selecting the first column and utilizing *length* to obtain it

- Create the survival vector with (by definition) survival probability of 1 in the first time instance

- Initilize a counter *j* to keep track of events occurring in the column of the dataset passed as the **events** parameter

- Iterate over the length of the dataset (*1:length(dataset[,1]*)) using *i* as iteration variable

    - During the loop we check if the i-th element of **events** does corespond to an event (1) or not (0)

    - If so, we add one to the counter and calculate the survival probability as a product of the previous survival probability obtained in the previous positive (1) event

        - We append the survival probability to the survival vector

    - 1 is subtracted from the total length of the dataset as one element in the dataset has been traversed. We use this *n_c* variable as a component of our survival probability calculation.

- Return the survival probability vector, the times where the survival probability changes correspond to the original time of events in the original passed on **dataset**.

```{r, echo=TRUE, warning=FALSE, message=FALSE}
km_est <- function(dataset, events) {
    # calculate survival probabilities
    n_c <- length(dataset[,1])
    survival <- c(1)
    j = 0
    for (i in 1:length(dataset[,1])) {
        if (events[i] == 1) {
            j = j+1
            prob <- (1-(1/n_c))*survival[j]
            survival <- c(survival, prob)
        }    
        n_c <- n_c - 1    
    }
    return(survival)
}
```

### Utilizing the function to obtain the survival function for the leukemia dataset

The survival probabilities are as follows, and these change over time at the times displayed on the *time* column of this table:

```{r, echo=FALSE, warning=FALSE, message=FALSE}
df <- aml[aml$x == 'Maintained',]
events <- df$status
survival <- km_est(dataset=df, events=events)
event_times <- df[df$status == 1,'time']
```
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=5}
# plotting the probs
j = 1
probs <- c()
for (i in 1:max(df$time)) {
    if (i %in% event_times) {
        j = j+1
    } 
    probs <- c(probs, survival[j])
}
probs <- data.frame(time=1:max(df$time), survival=probs)
ggplot(data=probs, aes(x=time, y=survival)) + geom_line()
```

## Exercise 4

### KM estimate of the survival function

We can see the estimate as follows:

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=5}
henning <- read.table('./Henning.txt', header=TRUE)
henning$censor = ifelse(henning$censor == 1,0,1)
henning$personal <- as.factor(henning$personal)
henning$property <- as.factor (henning$property)
fit <- survfit(Surv(henning$months, henning$censor)~1)
autoplot(fit)
```

In order to achieve this fit, we have modified the dataset as to convert the columns *personal* and *property* to factors, and then switch the 0s for 1s and vice versa in the *censor* column, given that these are reversed.

### Survival function: with crimes against persons

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=5}
henning2 <- henning[henning$personal==1,]
fit_persons1 <- survfit(Surv(henning2$months, henning2$censor)~1)
autoplot(fit_persons1)
```

### Survival function: without crimes against persons

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=5}
henning2 <- henning[henning$personal==0,]
fit_persons2 <- survfit(Surv(henning2$months, henning2$censor)~1)
autoplot(fit_persons2)
```

### Comparing both curves

We can see that the curve for nonpersonal crimes decays faster overall, as opposed to the personal crimes.

&nbsp;

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=5}
# plot both curves
fit=survfit(Surv(months,censor)~personal,data=henning)
autoplot(fit)
```

### Low-rank test

According to the low-rank test. Where our null hypothesis is that both groups are the same. We obtain a p-val of ~0.02 which is <0.05 (setting our confidence level at 95%), therefore we reject the null hypothesis. Therefore there are differences among groups.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
survdiff(Surv(months,censor)~personal,data=henning)
```

### Survival function: with crimes against property

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=5}
henning2 <- henning[henning$property==1,]
fit_property1 <- survfit(Surv(henning2$months, henning2$censor)~1)
autoplot(fit_property1)
```

### Survival function: with crimes against property

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=5}
henning2 <- henning[henning$property==0,]
fit_property2 <- survfit(Surv(henning2$months, henning2$censor)~1)
autoplot(fit_property2)
```

### Comparing both curves

We can see that the curve for non-property related crimes overall decays significantly faster than the opposite one.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# plot both curves
fit=survfit(Surv(months,censor)~property,data=henning)
autoplot(fit)
```

### Low-rank test

```{r, echo=FALSE, warning=FALSE, message=FALSE}
survdiff(Surv(months,censor)~property,data=henning)
```

As before, we must reject the null hypothesis. Therefore we can say there is a significant statistical difference among both groups.

### Fitting a Cox regression

Converting personal and property to leveled factors with labels yes/no.

\footnotesize

```{r, echo=TRUE, warning=FALSE, message=FALSE}
henning$personal=factor(henning$personal,levels=c("0","1"),labels=c("no","yes"))
henning$property=factor(henning$property,levels=c("0","1"),labels=c("no","yes"))
head(henning)
```

\normalsize

Running the cox regression fit.

\footnotesize

```{r, echo=TRUE, warning=FALSE, message=FALSE}
fit.all = coxph(Surv(months,censor) ~ cage + personal + property , henning)
summary(fit.all)
```

\normalsize

According to the p-value on our Wald test, we can see that all three varaibles are significant. With cage holding the largest significance by a lot. 

We can notice that for individuals which committed personal and property crimes, there is a positive increase in hazard, with property crimes yielding the largest increase in hazard.

As for cage, there is a small decrease in hazard given a higher value of *cage*. Hazard is associated with reincidence in crime.

This can be visualized in the *ggforest* plot from the *survminer* library.

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=6}
ggforest(fit.all, data=henning)
```

We can see that while personal crimes increase the probability of reincidence significantly, property crimes increase it to even a larger extent.

## Exercise 5

Given a hazard function $h(t) = c$, where $c > 0$:

We obtain the cumulative hazard function $H(t)$:

$$\begin{split} H(t) & = \int_0^{t} h(u) du \\
& = c \int_0^{t} du \\
& = ct \end{split}$$

With this, we derive the survival function $S(t)$:

$$\begin{split} H(t) & = ct \\ H(t) & = - log(S(t)) \\ ct & = -log(S(t)) \\ S(t) & = e^{-ct} \end{split}$$

And then we obtain the density function $f(t)$:

$$\begin{split} h(t) & = \frac{f(t)}{S(t)} \\
c & = \frac{f(t)}{e^{-ct}} \\
f(t) & = c e^{-ct} \end{split}$$

### Calculating median failure time with $c = 5$

We note the functions with $c = 5$ are:

$$\begin{split} h(t) & = 5 \\ H(t) & = 5t \\ S(t) & = e^{-5t} \\ f(t) & = 5 e^{-5t} \end{split}$$ 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# number of trials
len <- 1e5
range <- seq(1,100,length.out=len)
theta <- sapply(rep(5, len), function(h) {1/h})
cumulative_hazard <- theta*range
survival <- (1/theta) * exp(- cumulative_hazard)
df <- data.frame(time=range, survival=survival)
ggplot(df, aes(x=time, y=survival)) + geom_line() + ggtitle('Survival function')
```

The median failure time, according to 10,000 simulations, with a max length of time of 100, is as follows:

```{r, echo=FALSE, warning=FALSE, message=FALSE}
sim <- sapply(theta, function(t) {rexp(1,rate=1/t)})
median(sim)
```

Which we can visualize in a histogram:

```{r, echo=FALSE, warning=FALSE, message=FALSE}
hist(sim)
```

Therefore failure times are most commonly below 0.2 (~63% of the data) and almost always below 0.6 (~95% of the data).

## Exercise 6

First we read the data and remove missing values:

\footnotesize

```{r, echo=TRUE, warning=FALSE, message=FALSE}
# reading the data
lungcancer = read.table(file="http://www.mayo.edu/research/documents/lungdat/DOC-10027697", sep="",header=F, col.names=c("inst", "time", "status", "age", "sex", "ECOG","Karnofsky.physician", "Karnofsky.patient", "calories", "weight.loss"))
lungcancer[lungcancer=='.'] <- NA
lungcancer <- lungcancer[complete.cases(lungcancer), ]
```

\normalsize

Then we convert the corresponding variables into factors/numeric:

\footnotesize

```{r, echo=TRUE, warning=FALSE, message=FALSE}
factors <- c('status','sex','inst')
nums <- c('weight.loss','calories','Karnofsky.physician','Karnofsky.patient','ECOG')
for (name in factors) {lungcancer[,name] <- as.factor(lungcancer[,name])}
for (name in nums) {lungcancer[,name] <- as.numeric(lungcancer[,name])}
```

\normalsize

## Fitting a Cox PH model with all covariates

We fit the model and check its summary:

\tiny

```{r, echo=FALSE, warning=FALSE, message=FALSE}
fit <- coxph(Surv(time)~., data=lungcancer)
summary(fit)
```

\normalsize

## Wald test and LRT test for Karnofsky.patient and Karnofsky.physician

According the LRT and Wald test for this model, excluding vs including the variables shows that it is not statistically correct to simply remove these two variables. We cannot, with sufficient authority (and a confidence level of 95%), reject the null hypothesis. Therefore these two variables should remain in the model.

\tiny

```{r, echo=FALSE, warning=FALSE, message=FALSE}
fit_zonder <- coxph(Surv(time)~ . - Karnofsky.physician - Karnofsky.patient, data=lungcancer)
summary(fit_zonder)
```

\normalsize

## Finding the best Cox model

I will find the best model using stepBIC, so using the Bayesian Information Criterion in order to sequetially tune the model.

\tiny

```{r, echo=FALSE, warning=FALSE, message=FALSE}
lc_n <- dim(lungcancer)[1]
stepAIC(fit, k=log(lc_n))
```

\normalsize

The best model has a BIC of 1382.01.

The final model has the following formula: *Surv(time) ~ ECOG*

## Interpretation in terms of hazard ratios

\tiny

```{r, echo=FALSE, warning=FALSE, message=FALSE}
summary(coxph(Surv(time)~ECOG, data=lungcancer))
```

\normalsize

As we see in the summary, given that we're only left with the ECOG variable. We can see that the coefficient is 1.3076, therefore, the higher the ECOG score the higher the hazard, to a singificant extent, where basically 1 unit increase in ECOG score represents a ~31% increase in hazard.

And given the confidence intervals, the hazard ratio can be as low as 1.045 and as high as 1.636

